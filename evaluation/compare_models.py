#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¹æ¯”åˆ†æè„šæœ¬ï¼šåŸºç¡€æ¨¡å‹ vs Miniå¾®è°ƒ vs å®Œæ•´å¾®è°ƒ
"""

import os
import json
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from peft import PeftModel
from sacrebleu import corpus_bleu
from tqdm import tqdm
from huggingface_hub import login

def load_test_data(data_dir):
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    print(f"åŠ è½½æµ‹è¯•æ•°æ®ä»: {data_dir}")
    
    src_path = os.path.join(data_dir, "asm_Beng-eng_Latn", "test.asm_Beng")
    tgt_path = os.path.join(data_dir, "asm_Beng-eng_Latn", "test.eng_Latn")
    
    with open(src_path, "r", encoding="utf-8") as f:
        src_texts = [line.strip() for line in f if line.strip()]
    with open(tgt_path, "r", encoding="utf-8") as f:
        tgt_texts = [line.strip() for line in f if line.strip()]
    
    print(f"âœ“ åŠ è½½äº† {len(src_texts)} ä¸ªæµ‹è¯•æ ·æœ¬")
    return src_texts, tgt_texts

def translate_with_model(model, tokenizer, src_texts, src_lang="asm_Beng", tgt_lang="eng_Latn", batch_size=4):
    """ä½¿ç”¨æ¨¡å‹è¿›è¡Œç¿»è¯‘"""
    print(f"å¼€å§‹ç¿»è¯‘ {len(src_texts)} ä¸ªæ ·æœ¬...")
    translations = []
    
    for i in tqdm(range(0, len(src_texts), batch_size), desc="ç¿»è¯‘è¿›åº¦"):
        batch_src = src_texts[i:i+batch_size]
        batch_src_with_lang = [f"{src_lang} {tgt_lang} {src}" for src in batch_src]
        
        try:
            inputs = tokenizer(
                batch_src_with_lang,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=512,
                    num_beams=1,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    early_stopping=True
                )
            
            batch_translations = tokenizer.batch_decode(
                outputs,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )
            translations.extend(batch_translations)
        except Exception as e:
            print(f"ç¿»è¯‘ç¬¬ {i+1} ä¸ªæ ·æœ¬æ—¶å‡ºé”™: {e}")
            translations.extend([""] * len(batch_src))
    
    return translations

def calculate_bleu_score(predictions, references):
    """è®¡ç®—BLEUåˆ†æ•°"""
    bleu_score = corpus_bleu(predictions, [references])
    return bleu_score.score, str(bleu_score)

def test_base_model(base_model_name, src_texts, tgt_texts):
    """æµ‹è¯•åŸºç¡€æ¨¡å‹"""
    print("\n" + "="*50)
    print("ğŸ” æµ‹è¯•1: åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹")
    print("="*50)
    
    # ç™»å½•Hugging Face
    token = os.getenv("HUGGINGFACE_HUB_TOKEN")
    if token:
        login(token=token)
        print("âœ“ Hugging Faceç™»å½•æˆåŠŸ")
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    print(f"åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_name}")
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
    model = AutoModelForSeq2SeqLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    model.eval()
    print("âœ“ åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # ç¿»è¯‘
    predictions = translate_with_model(model, tokenizer, src_texts)
    
    # è®¡ç®—BLEUåˆ†æ•°
    bleu_score, bleu_details = calculate_bleu_score(predictions, tgt_texts)
    print(f"âœ“ åŸºç¡€æ¨¡å‹BLEUåˆ†æ•°: {bleu_score:.4f}")
    
    return {
        "model_type": "base_model",
        "bleu_score": bleu_score,
        "bleu_details": bleu_details,
        "predictions": predictions
    }

def test_mini_finetuned_model(base_model_name, lora_path, src_texts, tgt_texts):
    """æµ‹è¯•Miniå¾®è°ƒæ¨¡å‹"""
    print("\n" + "="*50)
    print("ğŸ” æµ‹è¯•2: Miniæ•°æ®é›†å¾®è°ƒæ¨¡å‹")
    print("="*50)
    
    # ç™»å½•Hugging Face
    token = os.getenv("HUGGINGFACE_HUB_TOKEN")
    if token:
        login(token=token)
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    print(f"åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_name}")
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
    model = AutoModelForSeq2SeqLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # åŠ è½½LoRAé€‚é…å™¨
    print(f"åŠ è½½LoRAé€‚é…å™¨: {lora_path}")
    model = PeftModel.from_pretrained(model, lora_path)
    model.eval()
    print("âœ“ Miniå¾®è°ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # ç¿»è¯‘
    predictions = translate_with_model(model, tokenizer, src_texts)
    
    # è®¡ç®—BLEUåˆ†æ•°
    bleu_score, bleu_details = calculate_bleu_score(predictions, tgt_texts)
    print(f"âœ“ Miniå¾®è°ƒæ¨¡å‹BLEUåˆ†æ•°: {bleu_score:.4f}")
    
    return {
        "model_type": "mini_finetuned",
        "bleu_score": bleu_score,
        "bleu_details": bleu_details,
        "predictions": predictions
    }

def test_full_finetuned_model(base_model_name, lora_path, src_texts, tgt_texts):
    """æµ‹è¯•å®Œæ•´å¾®è°ƒæ¨¡å‹"""
    print("\n" + "="*50)
    print("ğŸ” æµ‹è¯•3: å®Œæ•´æ•°æ®é›†å¾®è°ƒæ¨¡å‹")
    print("="*50)
    
    # ç™»å½•Hugging Face
    token = os.getenv("HUGGINGFACE_HUB_TOKEN")
    if token:
        login(token=token)
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    print(f"åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_name}")
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
    model = AutoModelForSeq2SeqLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # åŠ è½½LoRAé€‚é…å™¨
    print(f"åŠ è½½LoRAé€‚é…å™¨: {lora_path}")
    model = PeftModel.from_pretrained(model, lora_path)
    model.eval()
    print("âœ“ å®Œæ•´å¾®è°ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # ç¿»è¯‘
    predictions = translate_with_model(model, tokenizer, src_texts)
    
    # è®¡ç®—BLEUåˆ†æ•°
    bleu_score, bleu_details = calculate_bleu_score(predictions, tgt_texts)
    print(f"âœ“ å®Œæ•´å¾®è°ƒæ¨¡å‹BLEUåˆ†æ•°: {bleu_score:.4f}")
    
    return {
        "model_type": "full_finetuned",
        "bleu_score": bleu_score,
        "bleu_details": bleu_details,
        "predictions": predictions
    }

def generate_comparison_report(results, output_dir):
    """ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š"""
    print("\n" + "="*50)
    print("ğŸ“Š ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š")
    print("="*50)
    
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    with open(os.path.join(output_dir, "comparison_results.json"), "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=4)
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    report_path = os.path.join(output_dir, "comparison_report.txt")
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("=== IndicTrans2 é˜¿è¨å§†è¯­â†’è‹±è¯­æ¨¡å‹å¯¹æ¯”åˆ†ææŠ¥å‘Š ===\n\n")
        
        f.write("ğŸ“Š BLEUåˆ†æ•°å¯¹æ¯”:\n")
        for result in results:
            f.write(f"- {result['model_type']}: {result['bleu_score']:.4f}\n")
        
        f.write(f"\nğŸ“ˆ æ€§èƒ½æå‡åˆ†æ:\n")
        if len(results) >= 2:
            base_score = results[0]['bleu_score']
            mini_score = results[1]['bleu_score'] if len(results) > 1 else None
            full_score = results[2]['bleu_score'] if len(results) > 2 else None
            
            if mini_score:
                mini_improvement = ((mini_score - base_score) / base_score) * 100
                f.write(f"- Miniå¾®è°ƒæå‡: {mini_improvement:+.2f}% ({base_score:.4f} â†’ {mini_score:.4f})\n")
            
            if full_score:
                full_improvement = ((full_score - base_score) / base_score) * 100
                f.write(f"- å®Œæ•´å¾®è°ƒæå‡: {full_improvement:+.2f}% ({base_score:.4f} â†’ {full_score:.4f})\n")
                
                if mini_score:
                    mini_to_full = ((full_score - mini_score) / mini_score) * 100
                    f.write(f"- å®Œæ•´vs Mini: {mini_to_full:+.2f}% ({mini_score:.4f} â†’ {full_score:.4f})\n")
        
        f.write(f"\nğŸ¯ ç»“è®º:\n")
        f.write(f"- åŸºç¡€æ¨¡å‹: åŸå§‹ç¿»è¯‘èƒ½åŠ›\n")
        f.write(f"- Miniå¾®è°ƒ: å°æ•°æ®é›†å¿«é€Ÿé€‚åº”\n")
        f.write(f"- å®Œæ•´å¾®è°ƒ: æœ€ä½³ç¿»è¯‘æ€§èƒ½\n")
    
    print(f"âœ“ å¯¹æ¯”åˆ†ææŠ¥å‘Šä¿å­˜åˆ°: {report_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("=== IndicTrans2 æ¨¡å‹å¯¹æ¯”åˆ†æ ===")
    
    # è®¾ç½®å‚æ•°
    base_model = "ai4bharat/indictrans2-indic-en-dist-200M"
    test_data_dir = "data_processing/assamese_english_asm_eng_mini_format/test"
    mini_lora_path = "training/outputs/assamese_english_lora_asm_eng_20251021_145310"  # å½“å‰Miniå¾®è°ƒæ¨¡å‹
    full_lora_path = "training/outputs/assamese_english_lora_asm_eng_full_$(date +%Y%m%d_%H%M%S)"  # å®Œæ•´å¾®è°ƒæ¨¡å‹è·¯å¾„
    output_dir = "results/model_comparison"
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    src_texts, tgt_texts = load_test_data(test_data_dir)
    
    results = []
    
    # æµ‹è¯•1: åŸºç¡€æ¨¡å‹
    try:
        base_result = test_base_model(base_model, src_texts, tgt_texts)
        results.append(base_result)
    except Exception as e:
        print(f"âŒ åŸºç¡€æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•2: Miniå¾®è°ƒæ¨¡å‹
    try:
        mini_result = test_mini_finetuned_model(base_model, mini_lora_path, src_texts, tgt_texts)
        results.append(mini_result)
    except Exception as e:
        print(f"âŒ Miniå¾®è°ƒæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•3: å®Œæ•´å¾®è°ƒæ¨¡å‹ (å¦‚æœå­˜åœ¨)
    if os.path.exists(full_lora_path):
        try:
            full_result = test_full_finetuned_model(base_model, full_lora_path, src_texts, tgt_texts)
            results.append(full_result)
        except Exception as e:
            print(f"âŒ å®Œæ•´å¾®è°ƒæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
    else:
        print("âš ï¸ å®Œæ•´å¾®è°ƒæ¨¡å‹å°šæœªè®­ç»ƒï¼Œè·³è¿‡æµ‹è¯•")
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    generate_comparison_report(results, output_dir)
    
    print("\nğŸ‰ å¯¹æ¯”åˆ†æå®Œæˆï¼")
    print(f"ç»“æœä¿å­˜åœ¨: {output_dir}")

if __name__ == "__main__":
    main()
