#!/usr/bin/env python3
"""
è¯„ä¼°å¾®è°ƒåçš„é˜¿è¨å§†è¯­â†’è‹±è¯­æ¨¡å‹
"""

import os
import sys
import torch
import json
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from peft import PeftModel
from huggingface_hub import login
from sacrebleu import corpus_bleu
from tqdm import tqdm

def load_test_data(data_dir, src_lang="asm_Beng", tgt_lang="eng_Latn"):
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    print(f"åŠ è½½æµ‹è¯•æ•°æ®ä»: {data_dir}")
    src_path = os.path.join(data_dir, f"{src_lang}-{tgt_lang}", f"test.{src_lang}")
    tgt_path = os.path.join(data_dir, f"{src_lang}-{tgt_lang}", f"test.{tgt_lang}")

    if not os.path.exists(src_path) or not os.path.exists(tgt_path):
        raise FileNotFoundError(f"æµ‹è¯•æ–‡ä»¶æœªæ‰¾åˆ°åœ¨ {data_dir}")

    with open(src_path, "r", encoding="utf-8") as f:
        src_texts = [line.strip() for line in f if line.strip()]
    with open(tgt_path, "r", encoding="utf-8") as f:
        tgt_texts = [line.strip() for line in f if line.strip()]

    if len(src_texts) != len(tgt_texts):
        raise ValueError("æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„æµ‹è¯•æ ·æœ¬æ•°é‡ä¸åŒ¹é…ã€‚")

    print(f"âœ“ åŠ è½½äº† {len(src_texts)} ä¸ªæµ‹è¯•æ ·æœ¬")
    return src_texts, tgt_texts

def translate_batch(model, tokenizer, src_texts, batch_size=4):
    """æ‰¹é‡ç¿»è¯‘é˜¿è¨å§†è¯­åˆ°è‹±è¯­"""
    print(f"\nå¼€å§‹ç¿»è¯‘ {len(src_texts)} ä¸ªé˜¿è¨å§†è¯­å¥å­...")
    translations = []

    for i in tqdm(range(0, len(src_texts), batch_size), desc="ç¿»è¯‘è¿›åº¦"):
        batch_src = src_texts[i:i+batch_size]
        
        # æ·»åŠ è¯­è¨€æ ‡ç­¾ï¼šé˜¿è¨å§†è¯­â†’è‹±è¯­
        batch_src_with_lang = [f"asm_Beng eng_Latn {src}" for src in batch_src]
        
        try:
            inputs = tokenizer(
                batch_src_with_lang,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(model.device)

            # ç”Ÿæˆç¿»è¯‘
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=256,
                    num_beams=1,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )

            # è§£ç è¾“å‡º
            batch_translations = tokenizer.batch_decode(
                outputs,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )
            translations.extend(batch_translations)
        except Exception as e:
            print(f"ç¿»è¯‘ç¬¬ {i+1} ä¸ªæ ·æœ¬æ—¶å‡ºé”™: {e}")
            translations.extend([""] * len(batch_src))

    print(f"âœ“ ç¿»è¯‘å®Œæˆï¼Œç”Ÿæˆäº† {len(translations)} ä¸ªç¿»è¯‘ç»“æœ")
    return translations

def calculate_bleu_score(predictions, references):
    """è®¡ç®—BLEUåˆ†æ•°"""
    print(f"\nè®¡ç®—BLEUåˆ†æ•°...")
    
    # ä½¿ç”¨sacrebleuè®¡ç®—BLEUåˆ†æ•°
    bleu_score = corpus_bleu(predictions, [references])
    
    print(f"âœ“ BLEUåˆ†æ•°: {bleu_score.score:.4f}")
    print(f"âœ“ BLEUè¯¦ç»†ä¿¡æ¯: {bleu_score}")
    
    return bleu_score.score, str(bleu_score)

def calculate_other_metrics(predictions, references):
    """è®¡ç®—å…¶ä»–è¯„ä¼°æŒ‡æ ‡"""
    print(f"\nè®¡ç®—å…¶ä»–è¯„ä¼°æŒ‡æ ‡...")
    metrics = {}
    
    # å¹³å‡é¢„æµ‹é•¿åº¦
    pred_lengths = [len(p.split()) for p in predictions]
    ref_lengths = [len(r.split()) for r in references]
    
    avg_pred_len = sum(pred_lengths) / len(pred_lengths) if pred_lengths else 0
    avg_ref_len = sum(ref_lengths) / len(ref_lengths) if ref_lengths else 0
    
    metrics["average_prediction_length"] = avg_pred_len
    metrics["average_reference_length"] = avg_ref_len
    
    # é•¿åº¦æ¯”ç‡
    length_ratios = [
        (len(p.split()) / len(r.split())) if len(r.split()) > 0 else 0
        for p, r in zip(predictions, references)
    ]
    avg_length_ratio = sum(length_ratios) / len(length_ratios) if length_ratios else 0
    std_length_ratio = (
        (sum((x - avg_length_ratio) ** 2 for x in length_ratios) / len(length_ratios)) ** 0.5
        if length_ratios
        else 0
    )
    metrics["average_length_ratio"] = avg_length_ratio
    metrics["length_ratio_std_dev"] = std_length_ratio
    
    # å®Œå…¨åŒ¹é…ç‡
    exact_matches = sum(1 for p, r in zip(predictions, references) if p == r)
    exact_match_rate = exact_matches / len(predictions) if predictions else 0
    metrics["exact_match_rate"] = exact_match_rate
    
    print(f"âœ“ å¹³å‡é¢„æµ‹é•¿åº¦: {avg_pred_len:.2f}")
    print(f"âœ“ å¹³å‡å‚è€ƒé•¿åº¦: {avg_ref_len:.2f}")
    print(f"âœ“ å¹³å‡é•¿åº¦æ¯”ç‡: {avg_length_ratio:.4f}")
    print(f"âœ“ é•¿åº¦æ¯”ç‡æ ‡å‡†å·®: {std_length_ratio:.4f}")
    print(f"âœ“ å®Œå…¨åŒ¹é…ç‡: {exact_match_rate:.4f}")
    
    return metrics

def main():
    """ä¸»å‡½æ•°"""
    print("=== è¯„ä¼°å¾®è°ƒåçš„é˜¿è¨å§†è¯­â†’è‹±è¯­æ¨¡å‹ ===")
    
    # è®¾ç½®å‚æ•°
    base_model = "ai4bharat/indictrans2-indic-en-dist-200M"
    lora_model = "training/outputs/assamese_english_lora_asm_eng_20251021_145310"
    data_dir = "data_processing/assamese_english_asm_eng_mini_format/test"
    output_dir = "results/evaluation_results_asm_eng_test"
    
    print(f"åŸºç¡€æ¨¡å‹: {base_model}")
    print(f"LoRAæ¨¡å‹: {lora_model}")
    print(f"æµ‹è¯•æ•°æ®: {data_dir}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # ç™»å½•Hugging Face
    token = os.getenv("HUGGINGFACE_HUB_TOKEN")
    if token:
        login(token=token)
        print("âœ“ Hugging Faceç™»å½•æˆåŠŸ")
    
    # 1. åŠ è½½æµ‹è¯•æ•°æ®
    src_texts, tgt_texts = load_test_data(data_dir)
    
    # 2. åŠ è½½åŸºç¡€æ¨¡å‹å’Œtokenizer
    print(f"\nåŠ è½½åŸºç¡€æ¨¡å‹: {base_model}")
    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
    model = AutoModelForSeq2SeqLM.from_pretrained(
        base_model,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    print("âœ“ åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # 3. åŠ è½½LoRAé€‚é…å™¨
    print(f"\nåŠ è½½LoRAé€‚é…å™¨: {lora_model}")
    model = PeftModel.from_pretrained(model, lora_model)
    print("âœ“ LoRAé€‚é…å™¨åŠ è½½æˆåŠŸ")
    
    # 4. æ‰¹é‡ç¿»è¯‘
    predictions = translate_batch(model, tokenizer, src_texts)
    
    # 5. è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    bleu_score, bleu_details = calculate_bleu_score(predictions, tgt_texts)
    other_metrics = calculate_other_metrics(predictions, tgt_texts)
    
    # 6. ä¿å­˜è¯„ä¼°ç»“æœ
    os.makedirs(output_dir, exist_ok=True)
    results_df = pd.DataFrame({
        "source_asm": src_texts,
        "reference_eng": tgt_texts,
        "prediction_eng": predictions
    })
    results_df.to_csv(os.path.join(output_dir, "translation_results.csv"), index=False)
    print(f"âœ“ ç¿»è¯‘ç»“æœä¿å­˜åˆ°: {os.path.join(output_dir, 'translation_results.csv')}")
    
    evaluation_metrics = {
        "bleu_score": bleu_score,
        "bleu_details": bleu_details,
        **other_metrics
    }
    with open(os.path.join(output_dir, "evaluation_metrics.json"), "w", encoding="utf-8") as f:
        json.dump(evaluation_metrics, f, ensure_ascii=False, indent=4)
    print(f"âœ“ è¯„ä¼°æŒ‡æ ‡ä¿å­˜åˆ°: {os.path.join(output_dir, 'evaluation_metrics.json')}")
    
    # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    report_path = os.path.join(output_dir, "evaluation_report.txt")
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("=== å¾®è°ƒåçš„é˜¿è¨å§†è¯­â†’è‹±è¯­æ¨¡å‹è¯„ä¼°æŠ¥å‘Š ===\n")
        f.write(f"åŸºç¡€æ¨¡å‹: {base_model}\n")
        f.write(f"LoRAæ¨¡å‹: {lora_model}\n")
        f.write(f"æµ‹è¯•æ•°æ®: {data_dir}\n")
        f.write(f"è¾“å‡ºç›®å½•: {output_dir}\n\n")
        f.write(f"BLEUåˆ†æ•°: {bleu_score:.4f}\n")
        f.write(f"BLEUè¯¦ç»†ä¿¡æ¯:\n{bleu_details}\n\n")
        f.write("å…¶ä»–è¯„ä¼°æŒ‡æ ‡:\n")
        for key, value in other_metrics.items():
            f.write(f"- {key}: {value:.4f}\n")
        f.write("\nç¿»è¯‘æ ·æœ¬ (å‰10ä¸ª):\n")
        for i in range(min(10, len(src_texts))):
            f.write(f"  é˜¿è¨å§†è¯­: {src_texts[i]}\n")
            f.write(f"  å‚è€ƒè‹±è¯­: {tgt_texts[i]}\n")
            f.write(f"  é¢„æµ‹è‹±è¯­: {predictions[i]}\n\n")
    print(f"âœ“ è¯„ä¼°æŠ¥å‘Šä¿å­˜åˆ°: {report_path}")
    
    print("\nğŸ‰ è¯„ä¼°å®Œæˆï¼")
    print(f"BLEUåˆ†æ•°: {bleu_score:.4f}")
    print(f"ç»“æœä¿å­˜åœ¨: {output_dir}")

if __name__ == "__main__":
    main()
