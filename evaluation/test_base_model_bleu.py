#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹çš„BLEUåˆ†æ•°
è¿™æ˜¯å¯¹æ¯”åˆ†æçš„ç¬¬ä¸€æ­¥ï¼šå»ºç«‹åŸºçº¿æ€§èƒ½
"""

import os
import json
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from sacrebleu import corpus_bleu
from tqdm import tqdm
from huggingface_hub import login

def load_test_data(data_dir):
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    print(f"åŠ è½½æµ‹è¯•æ•°æ®ä»: {data_dir}")
    
    src_path = os.path.join(data_dir, "asm_Beng-eng_Latn", "test.asm_Beng")
    tgt_path = os.path.join(data_dir, "asm_Beng-eng_Latn", "test.eng_Latn")
    
    if not os.path.exists(src_path) or not os.path.exists(tgt_path):
        raise FileNotFoundError(f"æµ‹è¯•æ–‡ä»¶æœªæ‰¾åˆ°: {src_path} æˆ– {tgt_path}")
    
    with open(src_path, "r", encoding="utf-8") as f:
        src_texts = [line.strip() for line in f if line.strip()]
    with open(tgt_path, "r", encoding="utf-8") as f:
        tgt_texts = [line.strip() for line in f if line.strip()]
    
    if len(src_texts) != len(tgt_texts):
        raise ValueError(f"æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€æ ·æœ¬æ•°é‡ä¸åŒ¹é…: {len(src_texts)} vs {len(tgt_texts)}")
    
    print(f"âœ“ åŠ è½½äº† {len(src_texts)} ä¸ªæµ‹è¯•æ ·æœ¬")
    return src_texts, tgt_texts

def translate_with_base_model(model, tokenizer, src_texts, src_lang="asm_Beng", tgt_lang="eng_Latn", batch_size=4):
    """ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œç¿»è¯‘"""
    print(f"å¼€å§‹ç¿»è¯‘ {len(src_texts)} ä¸ªé˜¿è¨å§†è¯­å¥å­...")
    translations = []
    
    for i in tqdm(range(0, len(src_texts), batch_size), desc="ç¿»è¯‘è¿›åº¦"):
        batch_src = src_texts[i:i+batch_size]
        
        # æ·»åŠ è¯­è¨€æ ‡ç­¾ (IndicTrans2æ ¼å¼)
        batch_src_with_lang = [f"{src_lang} {tgt_lang} {src}" for src in batch_src]
        
        try:
            # ç¼–ç è¾“å…¥
            inputs = tokenizer(
                batch_src_with_lang,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(model.device)
            
            # ç”Ÿæˆç¿»è¯‘
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=512,
                    num_beams=1,  # ä½¿ç”¨greedy decoding
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    early_stopping=True
                )
            
            # è§£ç è¾“å‡º
            batch_translations = tokenizer.batch_decode(
                outputs,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )
            translations.extend(batch_translations)
            
        except Exception as e:
            print(f"ç¿»è¯‘ç¬¬ {i+1} ä¸ªæ ·æœ¬æ—¶å‡ºé”™: {e}")
            # æ·»åŠ ç©ºå­—ç¬¦ä¸²ä½œä¸ºå ä½ç¬¦
            translations.extend([""] * len(batch_src))
    
    print(f"âœ“ ç¿»è¯‘å®Œæˆï¼Œç”Ÿæˆäº† {len(translations)} ä¸ªç¿»è¯‘ç»“æœ")
    return translations

def calculate_bleu_score(predictions, references):
    """è®¡ç®—BLEUåˆ†æ•°"""
    print(f"\nè®¡ç®—BLEUåˆ†æ•°...")
    
    # ä½¿ç”¨sacrebleuè®¡ç®—BLEUåˆ†æ•°
    bleu_score = corpus_bleu(predictions, [references])
    
    print(f"âœ“ BLEUåˆ†æ•°: {bleu_score.score:.4f}")
    print(f"âœ“ BLEUè¯¦ç»†ä¿¡æ¯: {bleu_score}")
    
    return bleu_score.score, str(bleu_score)

def calculate_other_metrics(predictions, references):
    """è®¡ç®—å…¶ä»–è¯„ä¼°æŒ‡æ ‡"""
    print(f"\nè®¡ç®—å…¶ä»–è¯„ä¼°æŒ‡æ ‡...")
    metrics = {}
    
    # å¹³å‡é¢„æµ‹é•¿åº¦
    pred_lengths = [len(p.split()) for p in predictions]
    avg_pred_len = sum(pred_lengths) / len(pred_lengths) if pred_lengths else 0
    metrics["average_prediction_length"] = avg_pred_len
    
    # å¹³å‡å‚è€ƒé•¿åº¦
    ref_lengths = [len(r.split()) for r in references]
    avg_ref_len = sum(ref_lengths) / len(ref_lengths) if ref_lengths else 0
    metrics["average_reference_length"] = avg_ref_len
    
    # é•¿åº¦æ¯”ç‡
    length_ratios = [
        (len(p.split()) / len(r.split())) if len(r.split()) > 0 else 0
        for p, r in zip(predictions, references)
    ]
    avg_length_ratio = sum(length_ratios) / len(length_ratios) if length_ratios else 0
    std_length_ratio = (
        (sum((x - avg_length_ratio) ** 2 for x in length_ratios) / len(length_ratios)) ** 0.5
        if length_ratios
        else 0
    )
    metrics["average_length_ratio"] = avg_length_ratio
    metrics["length_ratio_std_dev"] = std_length_ratio
    
    # å®Œå…¨åŒ¹é…ç‡
    exact_matches = sum(1 for p, r in zip(predictions, references) if p == r)
    exact_match_rate = exact_matches / len(predictions) if predictions else 0
    metrics["exact_match_rate"] = exact_match_rate
    
    print(f"âœ“ å¹³å‡é¢„æµ‹é•¿åº¦: {avg_pred_len:.2f}")
    print(f"âœ“ å¹³å‡å‚è€ƒé•¿åº¦: {avg_ref_len:.2f}")
    print(f"âœ“ å¹³å‡é•¿åº¦æ¯”ç‡: {avg_length_ratio:.4f}")
    print(f"âœ“ é•¿åº¦æ¯”ç‡æ ‡å‡†å·®: {std_length_ratio:.4f}")
    print(f"âœ“ å®Œå…¨åŒ¹é…ç‡: {exact_match_rate:.4f}")
    
    return metrics

def save_results(predictions, references, bleu_score, bleu_details, other_metrics, output_dir):
    """ä¿å­˜ç»“æœ"""
    print(f"\nä¿å­˜ç»“æœåˆ°: {output_dir}")
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜ç¿»è¯‘ç»“æœ
    import pandas as pd
    results_df = pd.DataFrame({
        "source_asm": [""] * len(predictions),  # è¿™é‡Œå¯ä»¥æ·»åŠ æºè¯­è¨€æ–‡æœ¬
        "reference_eng": references,
        "prediction_eng": predictions
    })
    results_df.to_csv(os.path.join(output_dir, "base_model_translation_results.csv"), index=False)
    print(f"âœ“ ç¿»è¯‘ç»“æœä¿å­˜åˆ°: {os.path.join(output_dir, 'base_model_translation_results.csv')}")
    
    # ä¿å­˜è¯„ä¼°æŒ‡æ ‡
    evaluation_metrics = {
        "model_type": "base_model",
        "bleu_score": bleu_score,
        "bleu_details": bleu_details,
        **other_metrics
    }
    with open(os.path.join(output_dir, "base_model_evaluation_metrics.json"), "w", encoding="utf-8") as f:
        json.dump(evaluation_metrics, f, ensure_ascii=False, indent=4)
    print(f"âœ“ è¯„ä¼°æŒ‡æ ‡ä¿å­˜åˆ°: {os.path.join(output_dir, 'base_model_evaluation_metrics.json')}")
    
    # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    report_path = os.path.join(output_dir, "base_model_evaluation_report.txt")
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("=== IndicTrans2 åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹è¯„ä¼°æŠ¥å‘Š ===\n\n")
        f.write("ğŸ“Š æ¨¡å‹ä¿¡æ¯:\n")
        f.write("- æ¨¡å‹ç±»å‹: åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹ (æœªå¾®è°ƒ)\n")
        f.write("- æ¨¡å‹åç§°: ai4bharat/indictrans2-indic-en-dist-200M\n")
        f.write("- ç¿»è¯‘æ–¹å‘: é˜¿è¨å§†è¯­ â†’ è‹±è¯­\n\n")
        
        f.write("ğŸ“ˆ è¯„ä¼°ç»“æœ:\n")
        f.write(f"- BLEUåˆ†æ•°: {bleu_score:.4f}\n")
        f.write(f"- BLEUè¯¦ç»†ä¿¡æ¯: {bleu_details}\n\n")
        
        f.write("ğŸ“Š å…¶ä»–æŒ‡æ ‡:\n")
        for key, value in other_metrics.items():
            f.write(f"- {key}: {value:.4f}\n")
        
        f.write(f"\nğŸ“ ç¿»è¯‘æ ·æœ¬ (å‰5ä¸ª):\n")
        for i in range(min(5, len(predictions))):
            f.write(f"  å‚è€ƒ: {references[i]}\n")
            f.write(f"  é¢„æµ‹: {predictions[i]}\n\n")
    
    print(f"âœ“ è¯„ä¼°æŠ¥å‘Šä¿å­˜åˆ°: {report_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("=== IndicTrans2 åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹BLEUåˆ†æ•°æµ‹è¯• ===")
    
    # è®¾ç½®å‚æ•°
    base_model = "ai4bharat/indictrans2-indic-en-dist-200M"
    test_data_dir = "data_processing/assamese_english_asm_eng_mini_format/test"
    output_dir = "results/base_model_evaluation"
    src_lang = "asm_Beng"
    tgt_lang = "eng_Latn"
    batch_size = 4
    
    print(f"åŸºç¡€æ¨¡å‹: {base_model}")
    print(f"æµ‹è¯•æ•°æ®: {test_data_dir}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ç¿»è¯‘æ–¹å‘: {src_lang} â†’ {tgt_lang}")
    
    # 1. ç™»å½•Hugging Face
    token = os.getenv("HUGGINGFACE_HUB_TOKEN")
    if token:
        login(token=token)
        print("âœ“ Hugging Faceç™»å½•æˆåŠŸ")
    else:
        print("âš  æœªè®¾ç½®HUGGINGFACE_HUB_TOKENç¯å¢ƒå˜é‡ï¼Œå¯èƒ½æ— æ³•è®¿é—®ç§æœ‰æ¨¡å‹ã€‚")
    
    # 2. åŠ è½½æµ‹è¯•æ•°æ®
    src_texts, tgt_texts = load_test_data(test_data_dir)
    
    # 3. åŠ è½½åŸºç¡€æ¨¡å‹
    print(f"\nåŠ è½½åŸºç¡€æ¨¡å‹: {base_model}")
    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
    model = AutoModelForSeq2SeqLM.from_pretrained(
        base_model,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    model.eval()
    if torch.cuda.is_available():
        model = model.to("cuda")
        print("âœ“ æ¨¡å‹å·²åŠ è½½åˆ°GPU")
    else:
        print("âš  æœªæ£€æµ‹åˆ°GPUï¼Œæ¨¡å‹å°†åœ¨CPUä¸Šè¿è¡Œã€‚")
    print("âœ“ åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # 4. è¿›è¡Œç¿»è¯‘
    predictions = translate_with_base_model(model, tokenizer, src_texts, src_lang, tgt_lang, batch_size)
    
    # 5. è®¡ç®—BLEUåˆ†æ•°
    bleu_score, bleu_details = calculate_bleu_score(predictions, tgt_texts)
    
    # 6. è®¡ç®—å…¶ä»–æŒ‡æ ‡
    other_metrics = calculate_other_metrics(predictions, tgt_texts)
    
    # 7. ä¿å­˜ç»“æœ
    save_results(predictions, tgt_texts, bleu_score, bleu_details, other_metrics, output_dir)
    
    print("\nğŸ‰ åŸºç¡€æ¨¡å‹BLEUåˆ†æ•°æµ‹è¯•å®Œæˆï¼")
    print(f"BLEUåˆ†æ•°: {bleu_score:.4f}")
    print(f"ç»“æœä¿å­˜åœ¨: {output_dir}")
    
    return {
        "model_type": "base_model",
        "bleu_score": bleu_score,
        "bleu_details": bleu_details,
        **other_metrics
    }

if __name__ == "__main__":
    main()
